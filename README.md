# pages

# Running on Local Machine

In main directory (pages) folder open cmd and make a new environment, activate this environment and then install all these libraries in current environment. 

eg: 
- install required python and pip and then 
- python --version
- pip --verison (to verify installation)
- pip install virtualenv
- virtualenv pages (making virtual environment by 'pages' name)
- pages\Scripts\activate (by this command activate this environment)
After activation now you can install all requirements in this environment. 

Requirements:
- Python 3.6.7 (You can download this version by this link: https://www.python.org/ftp/python/3.6.7/python-3.6.7-amd64.exe)
- Pandas 0.22.0
- Matplotlib 2.2.2
- Argparse
- Pickle5
- Opencv-python 3.4.0.14
- xlrd 1.2.0
- xlsxwriter
- scikit-image
- Tensorflow v1.6.0 (If got error against protobuf then do this: 
				1. pip uninstall protobuf
				2. pip install protobuf==3.14.0
				3. Now install tensorflow (pip install tensorflow=1.6.0)	

Now make this directory structure. 

# Directory structure
<pre>
pages/
  PUCIT_OHUP
    -- train_pages/
    -- test_pages/
    -- train_lables_v3.xlsx
    -- test_lables_v3.xlsx
    -- generate_pickle_files.py <=== this file will read the PUCIT_OHUL dataset and populate the 'data/' folder with 7 pickle files (see below))
           Usage: python generate_pickle_files.py --valid_inds
    -- vocabulary.txt <=== will be generated by generate_pickle_files.py
    -- vocabulary_unicode.txt <=== will be generated by generate_pickle_files.py
  data/PUCIT_OHUP/
    -- train_pages.pkl
    -- valid_pages.pkl
    -- test_pages.pkl
    -- train_labels.pkl
    -- valid_labels.pkl
    -- test_labels.pkl
    -- vocabulary_unicode.pkl
  results/
    -- valid_predicted.txt
    -- valid_target.txt
    -- valid_wer.wer
    -- test_predicted.txt
    -- test_target.txt
    -- test_wer.wer
    -- log.txt
  chekpoints/
    -- cp-NUM-ACCURACY.ckpt  <== checkpoint files
  models/
    -- current_best_model.ckpt  <== checkpoint file for currently best performing model on validation set
</pre>


# How to train on dataset

PUCIT Offline Handwritten Urdu Pages (PUCIT-OHUP) Dataset and Ground Truth files:
  -- Train Pages images: https://drive.google.com/drive/folders/1VzOMEKPJDLgDjbQaaOSCsnhFLoAiuD1M?usp=sharing
  -- Test Pages images: https://drive.google.com/drive/folders/1iOiTdrqJVaNEn6EWa6xCfgTTw40dkhHM?usp=sharing
  -- Train Labels version 3: https://docs.google.com/spreadsheets/d/1TRnLxyoklFmBpYhLwJXZN7iGkzFsw0ju/edit?usp=sharing&ouid=110124311594051201443&rtpof=true&sd=true
  -- Test Labels version 3: https://docs.google.com/spreadsheets/d/1yvLNyWIMEZwifRz-v7mIAqkrCFJS0poo/edit?usp=sharing&ouid=110124311594051201443&rtpof=true&sd=true

# How to train on dataset
1. Place all training page images in the folder "train_pages"
2. Place all testing page images in the folder "test_pages"
3. Place all training ground-truth labels in the file "train_labels_v3.xlsx" using 2 columns. 
4. Place all testing ground-truth labels in the file "test_labels_v3.xlsx" using 2 columns. 
5. Run "generate_pickle_files.py" after specifying validation indices (if required). This will place all training, testing and validation images and labels in pickle format in the 'data/PUCIT_OHUP/' folder.
6. For training, set mode="train", and run "CALText.py". This will place the trained model(s) in 'models/' folder.
7. For testing, set mode="test", set path of the model to be used and run "CALText.py".
8. For Contextual Attention, set alpha_reg=1, while training.
9. For Contextual Attention Localization, set alpha_reg=0, while testing.

Now place these commands in current environment cmd. 

First you generate all pickle files by using this command: 

- python generate_pickle_files.py (make sure you should have in same directory where this file exists)

For train use this command:
- python CALText.py --mode=train --dataset=PUCIT_OHUP --alpha_reg=1
  
For test use this command: 
- python CALText.py --mode=test --dataset=PUCIT_OHUP --alpha_reg=0
